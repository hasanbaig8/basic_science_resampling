{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3129b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def get_logprobs(prompt: str, vllm_url: str = \"http://localhost:8000/v1/completions\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get logprobs for an input prompt without generation.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The input text to get logprobs for\n",
    "        vllm_url: URL of vLLM completions endpoint\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing logprobs information\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": \"Qwen/Qwen3-8b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 1,  # Minimal generation\n",
    "        \"temperature\": 0.0,\n",
    "        \"logprobs\": False,  # Request logprobs\n",
    "        \"echo\": True,  # Echo the prompt tokens with their logprobs\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(vllm_url, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "\n",
    "        log_probs = []\n",
    "        for token_dict in result['choices'][0]['prompt_logprobs'][1:]:\n",
    "            for _, logprob_dict in token_dict.items():\n",
    "                if (len(token_dict) == 1) or (logprob_dict['rank'] != 1):\n",
    "                    log_probs.append(logprob_dict['logprob'])\n",
    "        \n",
    "        return log_probs\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to get logprobs: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a229257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "prompt = \"The capital of Spain is\"\n",
    "logprobs = get_logprobs(prompt)\n",
    "\n",
    "# Print the logprobs information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b03d189c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-9.782947540283203,\n",
       " -0.5661647915840149,\n",
       " -6.395083904266357,\n",
       " -0.6938387155532837]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c1ff6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22371eab1c244ec982bb2275130d1096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f233433b3da4382a18b2fe31a47976c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48eacdd56d1a467c996aa5983e6645b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72948bbe48940fb9fd354edc576a85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fea3baedb94633a52ff410cebd3a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e479852a0e4e9682fc2636eceebfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/500 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0f56637dcd4c9ba3b6c6ad746019b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e3aa5188b74de0a22c89069dbb8179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7e8475f05948839f33b2aad20be284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Skywork/Skywork-Reward-V2-Qwen3-0.6B\n",
      "Model device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the Skywork Reward model\n",
    "model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loaded {model_name}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5300f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForSequenceClassification(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=1024, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f39235ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for response 1: 2.4263200759887695\n",
      "Score for response 2: -0.5910934805870056\n"
     ]
    }
   ],
   "source": [
    "# Test the reward model with two different responses\n",
    "prompt = \"Respond with a number between 0 and 100\"\n",
    "response1 = \"\"\"42\"\"\"\n",
    "response2 = \"\"\"200\"\"\"\n",
    "\n",
    "conv1 = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": response1}]\n",
    "conv2 = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": response2}]\n",
    "\n",
    "# Format and tokenize the conversations\n",
    "conv1_formatted = tokenizer.apply_chat_template(conv1, tokenize=False)\n",
    "conv2_formatted = tokenizer.apply_chat_template(conv2, tokenize=False)\n",
    "# These two lines remove the potential duplicate bos token\n",
    "if tokenizer.bos_token is not None and conv1_formatted.startswith(tokenizer.bos_token):\n",
    "    conv1_formatted = conv1_formatted[len(tokenizer.bos_token):]\n",
    "if tokenizer.bos_token is not None and conv2_formatted.startswith(tokenizer.bos_token):\n",
    "    conv2_formatted = conv2_formatted[len(tokenizer.bos_token):]\n",
    "conv1_tokenized = tokenizer(conv1_formatted, return_tensors=\"pt\").to('cuda')\n",
    "conv2_tokenized = tokenizer(conv2_formatted, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# Get the reward scores\n",
    "with torch.no_grad():\n",
    "    score1 = model(**conv1_tokenized).logits[0][0].item()\n",
    "    score2 = model(**conv2_tokenized).logits[0][0].item()\n",
    "print(f\"Score for response 1: {score1}\")\n",
    "print(f\"Score for response 2: {score2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1c73ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutputWithPast(loss=None, logits=tensor([[-3.7598]], device='cuda:0', grad_fn=<IndexBackward0>), past_key_values=None, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Start vLLM server for Qwen3 8B with 0.8 memory utilization\n",
    "from vllm import LLM, SamplingParams\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start vLLM server in background\n",
    "server_cmd = [\n",
    "    \"vllm\", \"serve\", \n",
    "    \"Qwen/Qwen2.5-8B-Instruct\",  # or appropriate Qwen3 8B model\n",
    "    \"--gpu-memory-utilization\", \"0.8\",\n",
    "    \"--host\", \"0.0.0.0\",\n",
    "    \"--port\", \"8000\"\n",
    "]\n",
    "\n",
    "# Start the server process\n",
    "server_process = subprocess.Popen(server_cmd)\n",
    "print(\"Starting vLLM server...\")\n",
    "time.sleep(30)  # Wait for server to start\n",
    "print(\"vLLM server should be running on http://localhost:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a0bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
