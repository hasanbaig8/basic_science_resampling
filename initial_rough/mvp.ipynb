{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbefe593",
   "metadata": {},
   "source": [
    "1. set up getting a completion from vllm server in the notebook\n",
    "2. set up getting incremental completion of 100 tokens at a time with 10 rollouts each. but just choose 0th each time\n",
    "3. assess each rollout by sentence embedding vector for \"angry\" or \"confident\" and output these scores\n",
    "4. manually verify that these scores make sense\n",
    "5. optimise choosing these scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d70cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt: <|im_start|>user\n",
      "The weather today is<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "First completion: <think>\n",
      "Okay, the user is asking about the weather today. I need to check if I have the current weather information. Since I'm a large language model, I can't access real-time data. I should inform the user that I can't provide the current weather.\n",
      "\n",
      "I should be polite and maybe suggest they check a weather app or website for the latest updates. Let me make sure my response is clear and helpful, even though I can't provide the current data.\n",
      "</think>\n",
      "\n",
      "I can't\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import requests\n",
    "import json\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6b\")\n",
    "\n",
    "def format_prompt(prompt):\n",
    "    \"\"\"Format prompt using chat template\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "def get_completions_from_vllm(prompt, max_tokens=100, temperature=0.7, n=10):\n",
    "    \"\"\"\n",
    "    Get completions from vLLM server running on port 8000 using completions API\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The text prompt to complete\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        temperature (float): Sampling temperature\n",
    "        n (int): Number of completions to generate\n",
    "    \n",
    "    Returns:\n",
    "        list: List of completion choices\n",
    "    \"\"\"\n",
    "    url = \"http://localhost:8000/v1/completions\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"Qwen/Qwen3-0.6b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": False,\n",
    "        \"n\": n\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        return result[\"choices\"]\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error connecting to vLLM server: {e}\")\n",
    "        return None\n",
    "    except (KeyError, IndexError) as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test basic completion\n",
    "test_prompt = \"The weather today is\"\n",
    "formatted_prompt = format_prompt(test_prompt)\n",
    "print(f\"Formatted prompt: {formatted_prompt}\")\n",
    "completions = get_completions_from_vllm(formatted_prompt)\n",
    "if completions:\n",
    "    print(f\"First completion: {completions[0]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e728f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading Qwen3 0.6b model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "Loading emotion examples from JSON file...\n",
      "Loaded 101 angry examples and 100 neutral examples\n",
      "\n",
      "Computing angry emotion embeddings from examples...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     68\u001b[39m angry_embeddings = []\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(angry_examples[:\u001b[32m10\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     embedding = \u001b[43mget_intermediate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     angry_embeddings.append(embedding)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAngry example \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[:\u001b[32m50\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m\u001b[33m - embedding shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mget_intermediate_embeddings\u001b[39m\u001b[34m(text, layer_idx)\u001b[39m\n\u001b[32m     36\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# Get all hidden states\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     hidden_states = outputs.hidden_states\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Extract from specified layer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:410\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    423\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    424\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    425\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:1023\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper.<locals>.make_capture_wrapper.<locals>.wrapped_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1021\u001b[39m         output = orig_forward(*args, **kwargs)\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     output = \u001b[43morig_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   1025\u001b[39m     collected_outputs[key] += (output,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:260\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:216\u001b[39m, in \u001b[36mQwen3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation != \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    214\u001b[39m     attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# diff with Llama\u001b[39;49;00m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    229\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/integrations/sdpa_attention.py:96\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.dtype != torch.bool:\n\u001b[32m     93\u001b[39m         \u001b[38;5;66;03m# Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\u001b[39;00m\n\u001b[32m     94\u001b[39m         attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msdpa_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Qwen3 0.6b model and tokenizer\n",
    "print(\"Loading Qwen3 0.6b model...\")\n",
    "model_name = \"Qwen/Qwen3-0.6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Set pad token if not exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def get_intermediate_embeddings(text, layer_idx=-3):\n",
    "    \"\"\"\n",
    "    Extract embeddings from an intermediate layer of Qwen3\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        layer_idx (int): Which layer to extract from (-1 is last, -2 is second to last, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Mean-pooled embeddings from the specified layer\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get all hidden states\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        \n",
    "        # Extract from specified layer\n",
    "        layer_embeddings = hidden_states[layer_idx]  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Mean pooling over sequence length (excluding padding tokens)\n",
    "        attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "        masked_embeddings = layer_embeddings * attention_mask\n",
    "        mean_embeddings = masked_embeddings.sum(dim=1) / attention_mask.sum(dim=1)\n",
    "        \n",
    "        # Normalize for cosine similarity\n",
    "        mean_embeddings = F.normalize(mean_embeddings, p=2, dim=1)\n",
    "        \n",
    "    return mean_embeddings\n",
    "\n",
    "# Load angry and neutral examples from JSON file\n",
    "print(\"Loading emotion examples from JSON file...\")\n",
    "with open('emotion_examples.json', 'r') as f:\n",
    "    emotion_data = json.load(f)\n",
    "\n",
    "angry_examples = emotion_data['angry']\n",
    "neutral_examples = emotion_data['neutral']\n",
    "\n",
    "print(f\"Loaded {len(angry_examples)} angry examples and {len(neutral_examples)} neutral examples\")\n",
    "\n",
    "# Use first 10 examples to create averaged embeddings\n",
    "print(\"\\nComputing angry emotion embeddings from examples...\")\n",
    "angry_embeddings = []\n",
    "for i, example in enumerate(angry_examples[:10]):\n",
    "    embedding = get_intermediate_embeddings(example)\n",
    "    angry_embeddings.append(embedding)\n",
    "    print(f\"Angry example {i+1}: '{example[:50]}...' - embedding shape: {embedding.shape}\")\n",
    "\n",
    "# Average all angry embeddings for a more robust representation\n",
    "angry_embedding = torch.mean(torch.stack(angry_embeddings), dim=0)\n",
    "print(f\"Final averaged angry embedding shape: {angry_embedding.shape}\")\n",
    "\n",
    "print(\"\\nComputing neutral emotion embeddings from examples...\")\n",
    "neutral_embeddings = []\n",
    "for i, example in enumerate(neutral_examples[:10]):\n",
    "    embedding = get_intermediate_embeddings(example)\n",
    "    neutral_embeddings.append(embedding)\n",
    "    print(f\"Neutral example {i+1}: '{example[:50]}...' - embedding shape: {embedding.shape}\")\n",
    "\n",
    "# Average all neutral embeddings for a more robust representation\n",
    "neutral_embedding = torch.mean(torch.stack(neutral_embeddings), dim=0)\n",
    "print(f\"Final averaged neutral embedding shape: {neutral_embedding.shape}\")\n",
    "\n",
    "def compute_emotion_score(text, target_embedding, neutral_embedding, layer_idx=-3):\n",
    "    \"\"\"\n",
    "    Compute emotion score as the difference between similarity to target emotion\n",
    "    and similarity to neutral baseline using intermediate layer representations from Qwen3\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to score\n",
    "        target_embedding (torch.Tensor): Target emotion embedding tensor (normalized)\n",
    "        neutral_embedding (torch.Tensor): Neutral baseline embedding tensor (normalized)\n",
    "        layer_idx (int): Which layer to extract embeddings from\n",
    "    \n",
    "    Returns:\n",
    "        float: Emotion score (target_similarity - neutral_similarity)\n",
    "    \"\"\"\n",
    "    text_embedding = get_intermediate_embeddings(text, layer_idx)\n",
    "    target_similarity = torch.mm(text_embedding, target_embedding.T).item()\n",
    "    neutral_similarity = torch.mm(text_embedding, neutral_embedding.T).item()\n",
    "    emotion_score = target_similarity - neutral_similarity\n",
    "    return emotion_score\n",
    "\n",
    "def compute_emotion_scores_batch(texts, target_embedding, neutral_embedding, layer_idx=-3):\n",
    "    \"\"\"\n",
    "    Compute emotion scores for multiple texts at once using Qwen3 intermediate layers\n",
    "    Score is the difference between target emotion similarity and neutral baseline similarity\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts to score\n",
    "        target_embedding (torch.Tensor): Target emotion embedding tensor (normalized)\n",
    "        neutral_embedding (torch.Tensor): Neutral baseline embedding tensor (normalized)\n",
    "        layer_idx (int): Which layer to extract embeddings from\n",
    "    \n",
    "    Returns:\n",
    "        list: List of emotion scores (target_similarity - neutral_similarity)\n",
    "    \"\"\"\n",
    "    # Tokenize all texts at once\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get all hidden states\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        \n",
    "        # Extract from specified layer\n",
    "        layer_embeddings = hidden_states[layer_idx]  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Mean pooling over sequence length (excluding padding tokens)\n",
    "        attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "        masked_embeddings = layer_embeddings * attention_mask\n",
    "        mean_embeddings = masked_embeddings.sum(dim=1) / attention_mask.sum(dim=1)\n",
    "        \n",
    "        # Normalize for cosine similarity\n",
    "        mean_embeddings = F.normalize(mean_embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Compute similarities to both target and neutral embeddings\n",
    "    target_similarities = torch.mm(mean_embeddings, target_embedding.T).squeeze()\n",
    "    neutral_similarities = torch.mm(mean_embeddings, neutral_embedding.T).squeeze()\n",
    "    \n",
    "    # Calculate emotion scores as difference\n",
    "    emotion_scores = (target_similarities - neutral_similarities).tolist()\n",
    "    \n",
    "    # Handle single text case\n",
    "    if isinstance(emotion_scores, float):\n",
    "        emotion_scores = [emotion_scores]\n",
    "    \n",
    "    return emotion_scores\n",
    "\n",
    "# Test the scoring function with varied examples\n",
    "test_texts = [\n",
    "    \"I can't believe you're asking me this again. Do you ever actually read my responses?\",\n",
    "    \"I can help you with that request. Let me provide the information you need.\",\n",
    "    \"This is absolutely ridiculous. I've explained this three times already!\",\n",
    "    \"That's an interesting question. I'll do my best to provide a helpful response.\",\n",
    "    \"You're driving me crazy with your complete lack of understanding!\",\n",
    "    \"Let me break down this concept for you in a clear way.\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Testing emotion scoring with JSON examples ===\")\n",
    "batch_scores = compute_emotion_scores_batch(test_texts, angry_embedding, neutral_embedding)\n",
    "for text, score in zip(test_texts, batch_scores):\n",
    "    emotion = \"ANGRY\" if score > 0 else \"NEUTRAL\"\n",
    "    print(f\"[{emotion}] Score: {score:.4f} | Text: '{text[:60]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e384ae35",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:412\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    411\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mLAZY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[32m    416\u001b[39m _tls.is_initializing = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "torch.ones(3).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f8fefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING RECURSIVE COMPLETION WITH ROLLOUT SCORING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def recursive_completion_with_rollouts(prompt, target_embedding, max_tokens=100, temperature=0.7, n_rollouts=10, max_iterations=10):\n",
    "    \"\"\"\n",
    "    Recursively generates completions with multiple rollouts at each step.\n",
    "    Evaluates all rollouts and selects the best one (currently just selecting 0th).\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The initial prompt\n",
    "        target_embedding (torch.Tensor): Target emotion embedding for scoring\n",
    "        max_tokens (int): Maximum tokens per completion request\n",
    "        temperature (float): Sampling temperature\n",
    "        n_rollouts (int): Number of rollouts to generate at each step\n",
    "        max_iterations (int): Maximum number of recursive calls\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (list of completion chunks, list of iteration data with scores)\n",
    "    \"\"\"\n",
    "    split_completions = []\n",
    "    assistant_prompt = \"\"\n",
    "    iteration = 0\n",
    "    iteration_data = []\n",
    "    \n",
    "    # Format the initial prompt once\n",
    "    formatted_prompt = format_prompt(prompt)\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ITERATION {iteration + 1}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Current assistant text length: {len(assistant_prompt)}\")\n",
    "        \n",
    "        # Build the continuation prompt by appending to the base formatted prompt\n",
    "        if assistant_prompt:\n",
    "            continuation_prompt = formatted_prompt + assistant_prompt\n",
    "        else:\n",
    "            continuation_prompt = formatted_prompt\n",
    "        \n",
    "        print(f\"Prompt suffix (last 100 chars): ...{continuation_prompt[-100:]}\")\n",
    "        \n",
    "        # Get completions for all rollouts\n",
    "        completions = get_completions_from_vllm(\n",
    "            continuation_prompt, \n",
    "            max_tokens=max_tokens, \n",
    "            temperature=temperature, \n",
    "            n=n_rollouts\n",
    "        )\n",
    "        \n",
    "        if not completions:\n",
    "            print(\"No completions received, stopping\")\n",
    "            break\n",
    "        \n",
    "        # Score each rollout\n",
    "        rollout_scores = []\n",
    "        print(f\"\\n--- Evaluating {len(completions)} rollouts ---\")\n",
    "        for idx, completion in enumerate(completions):\n",
    "            completion_text = completion[\"text\"]\n",
    "            finish_reason = completion[\"finish_reason\"]\n",
    "            \n",
    "            # Skip empty completions\n",
    "            if not completion_text or len(completion_text.strip()) == 0:\n",
    "                print(f\"Rollout {idx}: SKIPPED (empty completion)\")\n",
    "                continue\n",
    "            \n",
    "            # Compute emotion score\n",
    "            try:\n",
    "                score = compute_emotion_score(completion_text, target_embedding, neutral_embedding)\n",
    "            except Exception as e:\n",
    "                print(f\"Rollout {idx}: ERROR computing score - {e}\")\n",
    "                score = -999.0  # Assign very low score to errored completions\n",
    "            \n",
    "            rollout_scores.append({\n",
    "                \"rollout_idx\": idx,\n",
    "                \"text\": completion_text,\n",
    "                \"score\": score,\n",
    "                \"finish_reason\": finish_reason\n",
    "            })\n",
    "            \n",
    "            # Display summary\n",
    "            text_preview = completion_text[:80].replace('\\n', ' ')\n",
    "            if len(completion_text) > 80:\n",
    "                text_preview += \"...\"\n",
    "            print(f\"Rollout {idx}: score={score:.4f}, finish={finish_reason}, text='{text_preview}'\")\n",
    "        \n",
    "        # Check if we have any valid rollouts\n",
    "        if not rollout_scores:\n",
    "            print(\"No valid rollouts generated, stopping\")\n",
    "            break\n",
    "        \n",
    "        # Sort by score (descending)\n",
    "        rollout_scores.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        \n",
    "        print(f\"\\n--- Rollout Rankings (by angry score) ---\")\n",
    "        for rank, rollout in enumerate(rollout_scores[:5]):  # Show top 5\n",
    "            print(f\"Rank {rank+1}: Rollout {rollout['rollout_idx']} with score {rollout['score']:.4f}\")\n",
    "        \n",
    "        # For now, just choose the 0th rollout (as per requirement)\n",
    "        # Find rollout 0 in the scores\n",
    "        chosen_rollout_data = next((r for r in rollout_scores if r[\"rollout_idx\"] == 0), None)\n",
    "        \n",
    "        if chosen_rollout_data is None:\n",
    "            # If rollout 0 doesn't exist (was skipped), choose the best one\n",
    "            print(\"Warning: Rollout 0 was skipped, choosing best rollout instead\")\n",
    "            chosen_rollout_data = rollout_scores[0]\n",
    "        \n",
    "        chosen_text = chosen_rollout_data[\"text\"]\n",
    "        chosen_finish = chosen_rollout_data[\"finish_reason\"]\n",
    "        chosen_score = chosen_rollout_data[\"score\"]\n",
    "        chosen_rollout_idx = chosen_rollout_data[\"rollout_idx\"]\n",
    "        \n",
    "        print(f\"\\n--- CHOSEN: Rollout {chosen_rollout_idx} (score={chosen_score:.4f}) ---\")\n",
    "        print(f\"Text: {chosen_text[:150]}...\" if len(chosen_text) > 150 else f\"Text: {chosen_text}\")\n",
    "        print(f\"Finish reason: {chosen_finish}\")\n",
    "        \n",
    "        # Store iteration data\n",
    "        iteration_data.append({\n",
    "            \"iteration\": iteration + 1,\n",
    "            \"rollout_scores\": rollout_scores,\n",
    "            \"chosen_rollout_idx\": chosen_rollout_idx,\n",
    "            \"chosen_text\": chosen_text,\n",
    "            \"chosen_score\": chosen_score,\n",
    "            \"chosen_finish\": chosen_finish\n",
    "        })\n",
    "        \n",
    "        # Add to results\n",
    "        split_completions.append(chosen_text)\n",
    "        assistant_prompt += chosen_text\n",
    "        \n",
    "        # Check if we should stop\n",
    "        if chosen_finish == \"stop\":\n",
    "            print(\"\\nStopping due to finish_reason: stop\")\n",
    "            break\n",
    "            \n",
    "        iteration += 1\n",
    "    \n",
    "    return split_completions, iteration_data\n",
    "\n",
    "# Test the recursive completion with rollout scoring\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING RECURSIVE COMPLETION WITH ROLLOUT SCORING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ITERATION 1\n",
      "================================================================================\n",
      "Current assistant text length: 0\n",
      "Prompt suffix (last 100 chars): ...at are the most common emotions that people feel? Think for a while<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "--- Evaluating 30 rollouts ---\n",
      "Rollout 0: score=0.0075, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 1: score=0.0062, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 2: score=0.0062, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 3: score=0.0068, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 4: score=0.0075, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 5: score=0.0056, finish=length, text='<think> Okay, the user is asking what are the most common emotions people feel. ...'\n",
      "Rollout 6: score=0.0075, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 7: score=0.0064, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 8: score=0.0076, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 9: score=0.0058, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 10: score=0.0075, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 11: score=0.0075, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 12: score=0.0064, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 13: score=0.0068, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 14: score=0.0060, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 15: score=0.0071, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 16: score=0.0062, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 17: score=0.0077, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 18: score=0.0061, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 19: score=0.0064, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 20: score=0.0078, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 21: score=0.0065, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 22: score=0.0075, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 23: score=0.0071, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 24: score=0.0067, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 25: score=0.0068, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 26: score=0.0064, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 27: score=0.0075, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 28: score=0.0065, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "Rollout 29: score=0.0064, finish=length, text='<think> Okay, the user is asking about the most common emotions people feel. Let...'\n",
      "\n",
      "--- Rollout Rankings (by angry score) ---\n",
      "Rank 1: Rollout 20 with score 0.0078\n",
      "Rank 2: Rollout 17 with score 0.0077\n",
      "Rank 3: Rollout 8 with score 0.0076\n",
      "Rank 4: Rollout 0 with score 0.0075\n",
      "Rank 5: Rollout 6 with score 0.0075\n",
      "\n",
      "--- CHOSEN: Rollout 0 (score=0.0075) ---\n",
      "Text: <think>\n",
      "Okay, the user is asking about the most common emotions people feel. Let me start by recalling what I know. I remember that emotions can vary ...\n",
      "Finish reason: length\n",
      "\n",
      "================================================================================\n",
      "ITERATION 2\n",
      "================================================================================\n",
      "Current assistant text length: 246\n",
      "Prompt suffix (last 100 chars): ...ary widely, but there are some that are more prevalent. For example, happiness and sadness are often\n",
      "\n",
      "--- Evaluating 30 rollouts ---\n",
      "Rollout 0: score=0.0070, finish=length, text=' mentioned. But wait, are there more?  I think joy and anger are also common. Bu...'\n",
      "Rollout 1: score=0.0087, finish=length, text=' mentioned. But wait, are there others?  I think there's also fear, anger, and l...'\n",
      "Rollout 2: score=0.0093, finish=length, text=' mentioned. But wait, are there others? Maybe anger and fear? I think those are ...'\n",
      "Rollout 3: score=0.0063, finish=length, text=' mentioned. But maybe there are others too.  I should consider different cultura...'\n",
      "Rollout 4: score=0.0080, finish=length, text=' mentioned. But maybe I should check if there are more common ones.  I think pos...'\n",
      "Rollout 5: score=0.0069, finish=length, text=' mentioned. But wait, are there others?  I think there's also fear, anger, and l...'\n",
      "Rollout 6: score=0.0085, finish=length, text=' mentioned. But wait, are there others?  I think there's also fear, anger, and l...'\n",
      "Rollout 7: score=0.0070, finish=length, text=' mentioned. But wait, are there others? Maybe anger and fear? Also, anxiety and ...'\n",
      "Rollout 8: score=0.0067, finish=length, text=' mentioned. But wait, are there others? Maybe anger and fear? Also, anxiety and ...'\n",
      "Rollout 9: score=0.0087, finish=length, text=' mentioned. But wait, are there others? Maybe anger and fear? Or maybe anxiety a...'\n",
      "Rollout 10: score=0.0091, finish=length, text=' mentioned. But maybe there are others too. Let me think of some examples.  Happ...'\n",
      "Rollout 11: score=0.0065, finish=length, text=' mentioned. But wait, are there others? Maybe anger and fear? Also, anxiety and ...'\n",
      "Rollout 12: score=0.0066, finish=length, text=' mentioned. But wait, maybe there's more to it. I think anger and fear are also ...'\n",
      "Rollout 13: score=0.0069, finish=length, text=' mentioned. But wait, maybe there are others too. Like fear, anger, and loneline...'\n",
      "Rollout 14: score=0.0073, finish=length, text=' mentioned. But wait, are there others?  I think anger and fear are also common....'\n",
      "Rollout 15: score=0.0073, finish=length, text=' mentioned. But wait, are there others? Maybe anger and fear? Or maybe anxiety? ...'\n",
      "Rollout 16: score=0.0081, finish=length, text=' mentioned. But wait, are there more? Maybe anger and fear? Also, loneliness and...'\n",
      "Rollout 17: score=0.0090, finish=length, text=' mentioned. But wait, maybe there's more. Like anger and fear? Or maybe anxiety ...'\n",
      "Rollout 18: score=0.0090, finish=length, text=' mentioned. But wait, are there more? Maybe anger and fear? Or maybe anxiety and...'\n",
      "Rollout 19: score=0.0095, finish=length, text=' mentioned. But wait, are there more?  I think there's also fear, anger, and lon...'\n",
      "Rollout 20: score=0.0083, finish=length, text=' mentioned. But wait, are there more?  I think there are emotions like fear, ang...'\n",
      "Rollout 21: score=0.0103, finish=length, text=' mentioned. But wait, are there more? Like anger or fear? I think anger is also ...'\n",
      "Rollout 22: score=0.0087, finish=length, text=' mentioned. But wait, maybe I should think deeper. Are there other emotions that...'\n",
      "Rollout 23: score=0.0096, finish=length, text=' mentioned. But wait, maybe there are more. Like fear, anger, and loneliness? Or...'\n",
      "Rollout 24: score=0.0077, finish=length, text=' mentioned. But wait, are there more?  I think there's also fear and anger. Mayb...'\n",
      "Rollout 25: score=0.0093, finish=length, text=' mentioned. But wait, maybe there are more. Like anger, fear, or love? I should ...'\n",
      "Rollout 26: score=0.0084, finish=length, text=' mentioned. But maybe there are others too.  Wait, I should think about the most...'\n",
      "Rollout 27: score=0.0092, finish=length, text=' mentioned. But wait, are there more?  I think there's also fear and anger. Mayb...'\n",
      "Rollout 28: score=0.0086, finish=length, text=' mentioned. But wait, are there others? Maybe anger or fear? I think anger is al...'\n",
      "Rollout 29: score=0.0078, finish=length, text=' mentioned. But wait, are there others?  I think there's also fear and anger. Bu...'\n",
      "\n",
      "--- Rollout Rankings (by angry score) ---\n",
      "Rank 1: Rollout 21 with score 0.0103\n",
      "Rank 2: Rollout 23 with score 0.0096\n",
      "Rank 3: Rollout 19 with score 0.0095\n",
      "Rank 4: Rollout 25 with score 0.0093\n",
      "Rank 5: Rollout 2 with score 0.0093\n",
      "\n",
      "--- CHOSEN: Rollout 0 (score=0.0070) ---\n",
      "Text:  mentioned. But wait, are there more?\n",
      "\n",
      "I think joy and anger are also common. But maybe the user wants a more comprehensive list. Let me think of othe...\n",
      "Finish reason: length\n",
      "\n",
      "================================================================================\n",
      "ITERATION 3\n",
      "================================================================================\n",
      "Current assistant text length: 477\n",
      "Prompt suffix (last 100 chars): ...et me think of other emotions too. Fear, anxiety, and fear are definitely common. But maybe the user\n",
      "\n",
      "--- Evaluating 30 rollouts ---\n",
      "Rollout 0: score=0.0072, finish=length, text=' is looking for the top 5 or so.   Wait, the user might be interested in the mos...'\n",
      "Rollout 1: score=0.0055, finish=length, text=' is looking for the top 5 or so.   Wait, I should also consider cultural differe...'\n",
      "Rollout 2: score=0.0055, finish=length, text=' is looking for the most frequent ones.   Wait, I should check if there are any ...'\n",
      "Rollout 3: score=0.0030, finish=length, text=' is looking for the top five.   Wait, the user might be interested in understand...'\n",
      "Rollout 4: score=0.0020, finish=length, text=' is looking for a more general answer.   Wait, the user might be interested in u...'\n",
      "Rollout 5: score=0.0050, finish=length, text=' is looking for a broader range.   Wait, I should also consider cultural differe...'\n",
      "Rollout 6: score=0.0067, finish=length, text=' is looking for the top five.   Wait, I should check if there's a standard answe...'\n",
      "Rollout 7: score=0.0067, finish=length, text=' is looking for the most frequent ones.   Wait, I should check if there's a stan...'\n",
      "Rollout 8: score=0.0056, finish=length, text=' is looking for the most frequently reported ones.   Wait, some sources say that...'\n",
      "Rollout 9: score=0.0046, finish=length, text=' is looking for a more general answer.   I should also consider cultural differe...'\n",
      "Rollout 10: score=0.0044, finish=length, text=' is looking for the top 5 or 10.   I should also consider cultural differences. ...'\n",
      "Rollout 11: score=0.0046, finish=length, text=' is looking for a list that's more balanced.   Wait, there's also love and loss....'\n",
      "Rollout 12: score=0.0055, finish=length, text=' is looking for the most frequently reported ones.   Wait, I should check if the...'\n",
      "Rollout 13: score=0.0041, finish=length, text=' is looking for the top 5 or so.   Wait, the user might be interested in the mos...'\n",
      "Rollout 14: score=0.0069, finish=length, text=' is looking for the most frequent ones.   Wait, some people might feel lonelines...'\n",
      "Rollout 15: score=0.0061, finish=length, text=' is looking for the top 5.   Wait, I should check if there's a standard answer. ...'\n",
      "Rollout 16: score=0.0081, finish=length, text=' is looking for the most frequent ones.   Wait, happiness is a big one. Sadness ...'\n",
      "Rollout 17: score=0.0086, finish=length, text=' is looking for the top five.   Wait, some people might feel love and loneliness...'\n",
      "Rollout 18: score=0.0075, finish=length, text=' is looking for the most frequent ones.   Wait, some people might feel lonelines...'\n",
      "Rollout 19: score=0.0039, finish=length, text=' is looking for the top 5 or so.   Wait, the user didn't specify the number, so ...'\n",
      "Rollout 20: score=0.0033, finish=length, text=' is looking for the top five.   Wait, the user might be interested in understand...'\n",
      "Rollout 21: score=0.0063, finish=length, text=' is looking for the most frequent ones.   Wait, there's also loneliness, which i...'\n",
      "Rollout 22: score=0.0024, finish=length, text=' is looking for a broader range.   Wait, the user might be interested in the mos...'\n",
      "Rollout 23: score=0.0032, finish=length, text=' is looking for the top 5 or so.   Wait, the user might be interested in underst...'\n",
      "Rollout 24: score=0.0041, finish=length, text=' is looking for the most frequently experienced ones.   Wait, the question is ab...'\n",
      "Rollout 25: score=0.0068, finish=length, text=' is looking for the most frequently reported ones.   Wait, I should check if the...'\n",
      "Rollout 26: score=0.0059, finish=length, text=' is looking for the top 5 or 10.   Wait, I should check if there's a consensus o...'\n",
      "Rollout 27: score=0.0059, finish=length, text=' is looking for the most frequent ones.   Wait, the user might be interested in ...'\n",
      "Rollout 28: score=0.0039, finish=length, text=' is looking for the top 5 or so.   Wait, the user didn't specify the number, so ...'\n",
      "Rollout 29: score=0.0061, finish=length, text=' is looking for the most frequent ones.   Wait, maybe the answer should be struc...'\n",
      "\n",
      "--- Rollout Rankings (by angry score) ---\n",
      "Rank 1: Rollout 17 with score 0.0086\n",
      "Rank 2: Rollout 16 with score 0.0081\n",
      "Rank 3: Rollout 18 with score 0.0075\n",
      "Rank 4: Rollout 0 with score 0.0072\n",
      "Rank 5: Rollout 14 with score 0.0069\n",
      "\n",
      "--- CHOSEN: Rollout 0 (score=0.0072) ---\n",
      "Text:  is looking for the top 5 or so. \n",
      "\n",
      "Wait, the user might be interested in the most frequent emotions in everyday life. So, happiness is a big one. Then...\n",
      "Finish reason: length\n",
      "\n",
      "================================================================================\n",
      "ITERATION 4\n",
      "================================================================================\n",
      "Current assistant text length: 679\n",
      "Prompt suffix (last 100 chars): ... everyday life. So, happiness is a big one. Then maybe sadness, anger, fear, joy, and anxiety. But I\n",
      "\n",
      "--- Evaluating 30 rollouts ---\n",
      "Rollout 0: score=0.0061, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 1: score=0.0044, finish=length, text=' should check if there's any other that's more common.   I think the answer shou...'\n",
      "Rollout 2: score=0.0063, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 3: score=0.0081, finish=length, text=' should make sure not to include emotions that are less common. For example, may...'\n",
      "Rollout 4: score=0.0067, finish=length, text=' should check if there are any other emotions that are even more common.   I sho...'\n",
      "Rollout 5: score=0.0053, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 6: score=0.0061, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 7: score=0.0062, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 8: score=0.0081, finish=length, text=' should check if there are any other emotions that are more common. Like love an...'\n",
      "Rollout 9: score=0.0052, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 10: score=0.0060, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 11: score=0.0056, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 12: score=0.0053, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 13: score=0.0066, finish=length, text=' should check if there are any other emotions that are even more common.   I sho...'\n",
      "Rollout 14: score=0.0066, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 15: score=0.0054, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 16: score=0.0060, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 17: score=0.0065, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 18: score=0.0074, finish=length, text=' should check if there are any other emotions that are more common. Like love, b...'\n",
      "Rollout 19: score=0.0075, finish=length, text=' should check if there are any other emotions that are more common. Like lonelin...'\n",
      "Rollout 20: score=0.0039, finish=length, text=' should check if there are any others.   I should also consider cultural differe...'\n",
      "Rollout 21: score=0.0062, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 22: score=0.0059, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 23: score=0.0060, finish=length, text=' should check if there are other emotions that are more common.   I should also ...'\n",
      "Rollout 24: score=0.0053, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 25: score=0.0063, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 26: score=0.0061, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 27: score=0.0061, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 28: score=0.0052, finish=length, text=' should check if there are any other emotions that are more common.   I should a...'\n",
      "Rollout 29: score=0.0042, finish=length, text=' should make sure to mention that these are just examples and that there are man...'\n",
      "\n",
      "--- Rollout Rankings (by angry score) ---\n",
      "Rank 1: Rollout 8 with score 0.0081\n",
      "Rank 2: Rollout 3 with score 0.0081\n",
      "Rank 3: Rollout 19 with score 0.0075\n",
      "Rank 4: Rollout 18 with score 0.0074\n",
      "Rank 5: Rollout 4 with score 0.0067\n",
      "\n",
      "--- CHOSEN: Rollout 0 (score=0.0061) ---\n",
      "Text:  should check if there are any other emotions that are more common. \n",
      "\n",
      "I should also consider cultural factors. For example, in some cultures, anxiety ...\n",
      "Finish reason: length\n",
      "\n",
      "================================================================================\n",
      "ITERATION 5\n",
      "================================================================================\n",
      "Current assistant text length: 916\n",
      "Prompt suffix (last 100 chars): ...res, anxiety might be less common, but in others, it's more. However, the question is about the most\n",
      "\n",
      "--- Evaluating 30 rollouts ---\n",
      "Rollout 0: score=0.0068, finish=length, text=' common emotions, so maybe it's better to focus on the most universally recogniz...'\n",
      "Rollout 1: score=0.0030, finish=length, text=' common emotions in general, not cultural context.   So, the answer should list ...'\n",
      "Rollout 2: score=0.0071, finish=length, text=' common emotions, so maybe it's better to focus on universal ones.   Another tho...'\n",
      "Rollout 3: score=0.0063, finish=length, text=' common emotions, so maybe the answer should focus on the most general ones.   L...'\n",
      "Rollout 4: score=0.0059, finish=length, text=' common emotions, so maybe the answer should focus on the most universally recog...'\n",
      "Rollout 5: score=0.0040, finish=length, text=' common, so maybe the answer should focus on universal emotions.   Let me struct...'\n",
      "Rollout 6: score=0.0065, finish=length, text=' common emotions in general, not specific to culture.   Another thought: maybe t...'\n",
      "Rollout 7: score=0.0056, finish=length, text=' common, so maybe the answer should focus on the most widely recognized ones.   ...'\n",
      "Rollout 8: score=0.0055, finish=length, text=' common emotions in general, not specific to culture.   So, the answer should li...'\n",
      "Rollout 9: score=0.0047, finish=length, text=' common emotions in general, not specific to culture.   So, the answer should li...'\n",
      "Rollout 10: score=0.0065, finish=length, text=' common emotions, so maybe the answer should focus on the most universally recog...'\n",
      "Rollout 11: score=0.0069, finish=length, text=' common emotions, so maybe the answer should focus on the most universally recog...'\n",
      "Rollout 12: score=0.0063, finish=length, text=' common emotions, so maybe the answer should focus on the most universally recog...'\n",
      "Rollout 13: score=0.0051, finish=length, text=' common emotions, so maybe the answer should be based on general knowledge.   Le...'\n",
      "Rollout 14: score=0.0039, finish=length, text=' common emotions in general, not specific to culture.   So, the answer should li...'\n",
      "Rollout 15: score=0.0060, finish=length, text=' common emotions, so maybe the answer should focus on the general ones.   Let me...'\n",
      "Rollout 16: score=0.0049, finish=length, text=' common emotions in general, not specific to culture.   So, the answer should li...'\n",
      "Rollout 17: score=0.0070, finish=length, text=' common emotions, so maybe the answer should focus on the most widely recognized...'\n",
      "Rollout 18: score=0.0051, finish=length, text=' common emotions in general, not cultural context.   Another thought: emotions l...'\n",
      "Rollout 19: score=0.0046, finish=length, text=' common emotions in general, not cultural context.   So, the answer should list ...'\n",
      "Rollout 20: score=0.0047, finish=length, text=' common emotions in general, not specific to culture.   So, the answer should li...'\n",
      "Rollout 21: score=0.0056, finish=length, text=' common emotions, so maybe the answer should focus on the most widely recognized...'\n",
      "Rollout 22: score=0.0063, finish=length, text=' common emotions, so maybe the answer should be general.   Another thought: emot...'\n",
      "Rollout 23: score=0.0065, finish=length, text=' common emotions, so maybe the answer should focus on the most widely recognized...'\n",
      "Rollout 24: score=0.0047, finish=length, text=' common emotions in general, not cultural context.   So, the answer should list ...'\n",
      "Rollout 25: score=0.0057, finish=length, text=' common emotions, so maybe the answer should focus on the most widely recognized...'\n",
      "Rollout 26: score=0.0028, finish=length, text=' common emotions, so maybe the answer should be general.   Let me structure the ...'\n",
      "Rollout 27: score=0.0031, finish=length, text=' common emotions, so I should focus on the general ones.   Another thought: mayb...'\n",
      "Rollout 28: score=0.0055, finish=length, text=' common emotions in general, not cultural context.   So, the answer should list ...'\n",
      "Rollout 29: score=0.0040, finish=length, text=' common, so maybe the answer should focus on universal emotions.   Another thoug...'\n",
      "\n",
      "--- Rollout Rankings (by angry score) ---\n",
      "Rank 1: Rollout 2 with score 0.0071\n",
      "Rank 2: Rollout 17 with score 0.0070\n",
      "Rank 3: Rollout 11 with score 0.0069\n",
      "Rank 4: Rollout 0 with score 0.0068\n",
      "Rank 5: Rollout 6 with score 0.0065\n",
      "\n",
      "--- CHOSEN: Rollout 0 (score=0.0068) ---\n",
      "Text:  common emotions, so maybe it's better to focus on the most universally recognized ones. \n",
      "\n",
      "Hmm, maybe the answer should list happiness, sadness, joy, ...\n",
      "Finish reason: length\n",
      "\n",
      "================================================================================\n",
      "ITERATION 6\n",
      "================================================================================\n",
      "Current assistant text length: 1139\n",
      "Prompt suffix (last 100 chars): ...t happiness, sadness, joy, anger, fear, and anxiety. But I need to make sure these are the top ones.\n",
      "\n",
      "--- Evaluating 30 rollouts ---\n",
      "Rollout 0: score=0.0047, finish=length, text=' Let me confirm. Yes, those are the top emotions people experience. So the answe...'\n",
      "Rollout 1: score=0.0065, finish=length, text=' Let me verify. Yes, those are the top emotions people experience. So the answer...'\n",
      "Rollout 2: score=0.0064, finish=length, text=' Let me think again. Yes, those are the top emotions people experience. So the a...'\n",
      "Rollout 3: score=0.0026, finish=length, text=' Let me confirm. Yes, those are the top emotions. So the answer would be those f...'\n",
      "Rollout 4: score=0.0052, finish=length, text=' Let me confirm. Yes, those are the five most commonly reported emotions. So, th...'\n",
      "Rollout 5: score=0.0028, finish=length, text=' Also, maybe include a brief explanation of why they're common.   Wait, I should...'\n",
      "Rollout 6: score=0.0062, finish=length, text=' Let me verify. Yes, those are the five most common emotions. But wait, sometime...'\n",
      "Rollout 7: score=0.0051, finish=length, text=' Also, maybe include a brief explanation for each.   Wait, sometimes people feel...'\n",
      "Rollout 8: score=0.0066, finish=length, text=' Let me verify. Yes, those are the top emotions. So the answer would be those fi...'\n",
      "Rollout 9: score=0.0044, finish=length, text=' Let me verify. Yes, those are the top emotions. So the answer would be those fi...'\n",
      "Rollout 10: score=0.0068, finish=length, text=' Let me think again. Yes, those are the five most common emotions. But wait, is ...'\n",
      "Rollout 11: score=0.0058, finish=length, text=' Let me verify. Yes, these are the most common emotions people experience. So, t...'\n",
      "Rollout 12: score=0.0049, finish=length, text=' Let me verify. Yes, those are the most common emotions. So the answer would be ...'\n",
      "Rollout 13: score=0.0035, finish=length, text=' Let me think again. Yes, those are the five most common emotions. But maybe the...'\n",
      "Rollout 14: score=0.0049, finish=length, text=' Let me think again. Yes, those are the five most common emotions. But I should ...'\n",
      "Rollout 15: score=0.0046, finish=length, text=' Let me verify. Yes, those are the top emotions. So the answer would be those fi...'\n",
      "Rollout 16: score=0.0043, finish=length, text=' Let me verify. Yes, those are the top emotions. So the answer would be those fi...'\n",
      "Rollout 17: score=0.0063, finish=length, text=' Let me verify. Yes, those are the top emotions. So the answer would be happines...'\n",
      "Rollout 18: score=0.0052, finish=length, text=' Let me think again. Yes, those are the top emotions. So the answer would be tho...'\n",
      "Rollout 19: score=0.0032, finish=length, text=' Let me confirm. Yes, those are the top emotions. So the answer would be those f...'\n",
      "Rollout 20: score=0.0067, finish=length, text=' Let me confirm. Yes, those are the top emotions. So the answer would be those f...'\n",
      "Rollout 21: score=0.0059, finish=length, text=' Let me verify. Yes, these are the top emotions people experience. So, the answe...'\n",
      "Rollout 22: score=0.0051, finish=length, text=' Let me think again.   Yes, those are the top emotions. So the answer would be h...'\n",
      "Rollout 23: score=0.0056, finish=length, text=' Let me verify. Yes, those are the top emotions. So, the answer should include t...'\n",
      "Rollout 24: score=0.0071, finish=length, text=' Let me verify. Yes, those are the top emotions. So, the answer would be happine...'\n",
      "Rollout 25: score=0.0052, finish=length, text=' Let me verify. Yes, these are the top emotions people experience. So the answer...'\n",
      "Rollout 26: score=0.0039, finish=length, text=' Let me confirm. Yes, those are the top emotions. So, the answer would be those ...'\n",
      "Rollout 27: score=0.0065, finish=length, text=' Let me verify. Yes, these are the top emotions. So, the answer would be those f...'\n",
      "Rollout 28: score=0.0050, finish=length, text=' Let me confirm. Yes, those are the top emotions people experience. So the answe...'\n",
      "Rollout 29: score=0.0053, finish=length, text=' Let me confirm. Yes, those are the five most common emotions. But I should also...'\n",
      "\n",
      "--- Rollout Rankings (by angry score) ---\n",
      "Rank 1: Rollout 24 with score 0.0071\n",
      "Rank 2: Rollout 10 with score 0.0068\n",
      "Rank 3: Rollout 20 with score 0.0067\n",
      "Rank 4: Rollout 8 with score 0.0066\n",
      "Rank 5: Rollout 27 with score 0.0065\n",
      "\n",
      "--- CHOSEN: Rollout 0 (score=0.0047) ---\n",
      "Text:  Let me confirm. Yes, those are the top emotions people experience. So the answer should include those, perhaps with a brief explanation of why they'r...\n",
      "Finish reason: length\n",
      "\n",
      "================================================================================\n",
      "ITERATION 7\n",
      "================================================================================\n",
      "Current assistant text length: 1393\n",
      "Prompt suffix (last 100 chars): ...mmon.\n",
      "</think>\n",
      "\n",
      "The most common emotions people experience are often categorized as follows:\n",
      "\n",
      "1. **H\n",
      "\n",
      "--- Evaluating 30 rollouts ---\n",
      "Rollout 0: score=0.0106, finish=length, text='appiness** – Often described as feeling joy, satisfaction, or contentment.   2. ...'\n",
      "Rollout 1: score=0.0089, finish=length, text='appiness** – Often described as joy, it is a universal feeling of fulfillment an...'\n",
      "Rollout 2: score=0.0094, finish=length, text='appiness** – A deep sense of joy and fulfillment.   2. **Sadness** – A feeling o...'\n",
      "Rollout 3: score=0.0091, finish=length, text='appiness**: A deep sense of joy and satisfaction that comes from positive experi...'\n",
      "Rollout 4: score=0.0082, finish=length, text='appiness**: This is a universal emotion that comes from positive experiences, jo...'\n",
      "Rollout 5: score=0.0081, finish=length, text='appiness** – A fundamental and universal emotion that comes from joy, contentmen...'\n",
      "Rollout 6: score=0.0088, finish=length, text='appiness** – Often described as joy, it is a universal feeling of fulfillment an...'\n",
      "Rollout 7: score=0.0100, finish=length, text='appiness** – A natural feeling of joy, satisfaction, and fulfillment.   2. **Sad...'\n",
      "Rollout 8: score=0.0087, finish=length, text='appiness** – Often described as joy, it is a universal feeling of contentment an...'\n",
      "Rollout 9: score=0.0102, finish=length, text='appiness** – Often described as feeling joy, satisfaction, or fulfillment.   2. ...'\n",
      "Rollout 10: score=0.0087, finish=length, text='appiness**: A widespread feeling of joy and satisfaction, often tied to positive...'\n",
      "Rollout 11: score=0.0097, finish=length, text='appiness** – A universal feeling of joy and contentment.   2. **Sadness** – A na...'\n",
      "Rollout 12: score=0.0082, finish=length, text='appiness** – A natural and positive emotion that arises from joy, contentment, o...'\n",
      "Rollout 13: score=0.0081, finish=length, text='appiness** – A natural feeling of joy and contentment.   2. **Sadness** – A comm...'\n",
      "Rollout 14: score=0.0084, finish=length, text='appiness**: A universal feeling of joy and satisfaction, often linked to positiv...'\n",
      "Rollout 15: score=0.0088, finish=length, text='appiness** – Often described as joy, it is a universal feeling of contentment an...'\n",
      "Rollout 16: score=0.0082, finish=length, text='appiness** – A natural and positive emotion that arises from joy, contentment, o...'\n",
      "Rollout 17: score=0.0090, finish=length, text='appiness** – A natural and positive emotion that comes from joy, contentment, or...'\n",
      "Rollout 18: score=0.0098, finish=length, text='appiness** – A natural feeling of joy, satisfaction, and contentment.   2. **Sad...'\n",
      "Rollout 19: score=0.0100, finish=length, text='appiness**: A deep sense of joy and fulfillment.   2. **Sadness**: A feeling of ...'\n",
      "Rollout 20: score=0.0083, finish=length, text='appiness** – A natural and positive emotion that comes from joy, contentment, or...'\n",
      "Rollout 21: score=0.0096, finish=length, text='appiness** – A natural feeling of joy and contentment.   2. **Sadness** – A resp...'\n",
      "Rollout 22: score=0.0088, finish=length, text='appiness** – A natural feeling of joy and satisfaction, often linked to positive...'\n",
      "Rollout 23: score=0.0080, finish=length, text='appiness** – A natural feeling of joy and contentment, often tied to positive ex...'\n",
      "Rollout 24: score=0.0096, finish=length, text='appiness** – A natural feeling of joy and fulfillment.   2. **Sadness** – A resp...'\n",
      "Rollout 25: score=0.0102, finish=length, text='appiness** – Often described as feeling joy, satisfaction, or contentment.   2. ...'\n",
      "Rollout 26: score=0.0087, finish=length, text='appiness**: A deep sense of joy and satisfaction, often tied to positive experie...'\n",
      "Rollout 27: score=0.0096, finish=length, text='appiness** – A natural feeling of joy and fulfillment.   2. **Sadness** – A resp...'\n",
      "Rollout 28: score=0.0085, finish=length, text='appiness**: It is one of the most universally recognized emotions, often tied to...'\n",
      "Rollout 29: score=0.0088, finish=length, text='appiness** – Often described as joy, it is a universal feeling of contentment an...'\n",
      "\n",
      "--- Rollout Rankings (by angry score) ---\n",
      "Rank 1: Rollout 0 with score 0.0106\n",
      "Rank 2: Rollout 9 with score 0.0102\n",
      "Rank 3: Rollout 25 with score 0.0102\n",
      "Rank 4: Rollout 19 with score 0.0100\n",
      "Rank 5: Rollout 7 with score 0.0100\n",
      "\n",
      "--- CHOSEN: Rollout 0 (score=0.0106) ---\n",
      "Text: appiness** – Often described as feeling joy, satisfaction, or contentment.  \n",
      "2. **Sadness** – Commonly associated with loneliness, longing, or loss.  ...\n",
      "Finish reason: length\n",
      "\n",
      "================================================================================\n",
      "ITERATION 8\n",
      "================================================================================\n",
      "Current assistant text length: 1609\n",
      "Prompt suffix (last 100 chars): ...th loneliness, longing, or loss.  \n",
      "3. **Anger** – Often triggered by frustration, disappointment, or\n",
      "\n",
      "--- Evaluating 30 rollouts ---\n",
      "Rollout 0: score=0.0073, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or threat...'\n",
      "Rollout 1: score=0.0073, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 2: score=0.0075, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 3: score=0.0074, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 4: score=0.0076, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, threat, or uncertainty...'\n",
      "Rollout 5: score=0.0073, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 6: score=0.0073, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or threat...'\n",
      "Rollout 7: score=0.0074, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or threat...'\n",
      "Rollout 8: score=0.0073, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 9: score=0.0074, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or threat...'\n",
      "Rollout 10: score=0.0074, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 11: score=0.0074, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 12: score=0.0073, finish=length, text=' injustice.   4. **Fear** – Commonly linked to uncertainty, danger, or anxiety. ...'\n",
      "Rollout 13: score=0.0074, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 14: score=0.0074, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or threat...'\n",
      "Rollout 15: score=0.0073, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 16: score=0.0072, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or threat...'\n",
      "Rollout 17: score=0.0075, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 18: score=0.0074, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 19: score=0.0074, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 20: score=0.0075, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, threat, or uncertainty...'\n",
      "Rollout 21: score=0.0079, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or threat...'\n",
      "Rollout 22: score=0.0074, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 23: score=0.0074, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 24: score=0.0075, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, threat, or uncertainty...'\n",
      "Rollout 25: score=0.0073, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 26: score=0.0073, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 27: score=0.0085, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, threat, or uncertainty...'\n",
      "Rollout 28: score=0.0073, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "Rollout 29: score=0.0073, finish=length, text=' injustice.   4. **Fear** – A natural response to danger, uncertainty, or anxiet...'\n",
      "\n",
      "--- Rollout Rankings (by angry score) ---\n",
      "Rank 1: Rollout 27 with score 0.0085\n",
      "Rank 2: Rollout 21 with score 0.0079\n",
      "Rank 3: Rollout 4 with score 0.0076\n",
      "Rank 4: Rollout 24 with score 0.0075\n",
      "Rank 5: Rollout 2 with score 0.0075\n",
      "\n",
      "--- CHOSEN: Rollout 0 (score=0.0073) ---\n",
      "Text:  injustice.  \n",
      "4. **Fear** – A natural response to danger, uncertainty, or threat.  \n",
      "5. **Joy** – A positive emotion that comes from happiness, fulfill...\n",
      "Finish reason: length\n",
      "\n",
      "================================================================================\n",
      "ITERATION 9\n",
      "================================================================================\n",
      "Current assistant text length: 1826\n",
      "Prompt suffix (last 100 chars): ...hat comes from happiness, fulfillment, or a sense of purpose.  \n",
      "\n",
      "These emotions are deeply rooted in\n",
      "\n",
      "--- Evaluating 30 rollouts ---\n",
      "Rollout 0: score=0.0053, finish=stop, text=' human psychology and are frequently experienced in daily life. While emotions c...'\n",
      "Rollout 1: score=0.0052, finish=stop, text=' human experience and are frequently linked to daily life, relationships, and pe...'\n",
      "Rollout 2: score=0.0043, finish=stop, text=' human experience and are often shared across cultures and individuals. While em...'\n",
      "Rollout 3: score=0.0040, finish=stop, text=' human psychology and are often intertwined with daily experiences. While indivi...'\n",
      "Rollout 4: score=0.0045, finish=stop, text=' human psychology and are frequently experienced in daily life. While individual...'\n",
      "Rollout 5: score=0.0051, finish=stop, text=' human psychology and are often intertwined with daily experiences. While emotio...'\n",
      "Rollout 6: score=0.0043, finish=stop, text=' human experience and are often shared across cultures and individuals. While so...'\n",
      "Rollout 7: score=0.0042, finish=stop, text=' human experience and are often shared across cultures and individuals. While em...'\n",
      "Rollout 8: score=0.0038, finish=stop, text=' human experience and are frequently observed in daily life. While individual ex...'\n",
      "Rollout 9: score=0.0046, finish=stop, text=' human experience and are often shared across cultures and personal contexts. Wh...'\n",
      "Rollout 10: score=0.0043, finish=stop, text=' human experience and are frequently shared across cultures and individuals. Whi...'\n",
      "Rollout 11: score=0.0051, finish=stop, text=' human experience and are often linked to daily life, relationships, and persona...'\n",
      "Rollout 12: score=0.0043, finish=stop, text=' human experience and are frequently observed in daily life. While individual ex...'\n",
      "Rollout 13: score=0.0043, finish=stop, text=' human experience and are frequently observed in daily life. While individual ex...'\n",
      "Rollout 14: score=0.0044, finish=stop, text=' human experience and are frequently observed in daily life. While individual ex...'\n",
      "Rollout 15: score=0.0050, finish=stop, text=' human psychology and are frequently experienced in daily life. While emotions c...'\n",
      "Rollout 16: score=0.0044, finish=stop, text=' human experience and are frequently shared across cultures and individuals. Whi...'\n",
      "Rollout 17: score=0.0043, finish=stop, text=' human experience and are frequently shared across cultures and individuals. Whi...'\n",
      "Rollout 18: score=0.0042, finish=stop, text=' human psychology and are frequently experienced in daily life. While individual...'\n",
      "Rollout 19: score=0.0045, finish=stop, text=' human psychology and are often intertwined with other experiences, making them ...'\n",
      "Rollout 20: score=0.0040, finish=stop, text=' human experience and are frequently observed in daily life. While individual ex...'\n",
      "Rollout 21: score=0.0042, finish=stop, text=' human experience and are frequently shared across cultures and individuals. Whi...'\n",
      "Rollout 22: score=0.0035, finish=stop, text=' human experience and are frequently encountered in daily life. While individual...'\n",
      "Rollout 23: score=0.0044, finish=stop, text=' human experience and are frequently shared by individuals across cultures and a...'\n",
      "Rollout 24: score=0.0048, finish=stop, text=' human psychology and are frequently experienced in daily life. While individual...'\n",
      "Rollout 25: score=0.0048, finish=stop, text=' human psychology and are frequently experienced in daily life. While individual...'\n",
      "Rollout 26: score=0.0046, finish=stop, text=' human experience and are frequently shared across cultures and individuals. Whi...'\n",
      "Rollout 27: score=0.0040, finish=stop, text=' human experience and are often intertwined with other aspects of life. While in...'\n",
      "Rollout 28: score=0.0039, finish=stop, text=' human experience and are frequently shared across cultures and individuals. Whi...'\n",
      "Rollout 29: score=0.0048, finish=stop, text=' human psychology and are frequently experienced in daily life. While individual...'\n",
      "\n",
      "--- Rollout Rankings (by angry score) ---\n",
      "Rank 1: Rollout 0 with score 0.0053\n",
      "Rank 2: Rollout 1 with score 0.0052\n",
      "Rank 3: Rollout 5 with score 0.0051\n",
      "Rank 4: Rollout 11 with score 0.0051\n",
      "Rank 5: Rollout 15 with score 0.0050\n",
      "\n",
      "--- CHOSEN: Rollout 0 (score=0.0053) ---\n",
      "Text:  human psychology and are frequently experienced in daily life. While emotions can vary across cultures and contexts, these are widely recognized as t...\n",
      "Finish reason: stop\n",
      "\n",
      "Stopping due to finish_reason: stop\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Chunk 1:\n",
      "<think>\n",
      "Okay, the user is asking about the most common emotions people feel. Let me start by recalling what I know. I remember that emotions can vary widely, but there are some that are more prevalent. For example, happiness and sadness are often\n",
      "\n",
      "Chunk 2:\n",
      " mentioned. But wait, are there more?\n",
      "\n",
      "I think joy and anger are also common. But maybe the user wants a more comprehensive list. Let me think of other emotions too. Fear, anxiety, and fear are definitely common. But maybe the user\n",
      "\n",
      "Chunk 3:\n",
      " is looking for the top 5 or so. \n",
      "\n",
      "Wait, the user might be interested in the most frequent emotions in everyday life. So, happiness is a big one. Then maybe sadness, anger, fear, joy, and anxiety. But I\n",
      "\n",
      "Chunk 4:\n",
      " should check if there are any other emotions that are more common. \n",
      "\n",
      "I should also consider cultural factors. For example, in some cultures, anxiety might be less common, but in others, it's more. However, the question is about the most\n",
      "\n",
      "Chunk 5:\n",
      " common emotions, so maybe it's better to focus on the most universally recognized ones. \n",
      "\n",
      "Hmm, maybe the answer should list happiness, sadness, joy, anger, fear, and anxiety. But I need to make sure these are the top ones.\n",
      "\n",
      "Chunk 6:\n",
      " Let me confirm. Yes, those are the top emotions people experience. So the answer should include those, perhaps with a brief explanation of why they're common.\n",
      "</think>\n",
      "\n",
      "The most common emotions people experience are often categorized as follows:\n",
      "\n",
      "1. **H\n",
      "\n",
      "Chunk 7:\n",
      "appiness** – Often described as feeling joy, satisfaction, or contentment.  \n",
      "2. **Sadness** – Commonly associated with loneliness, longing, or loss.  \n",
      "3. **Anger** – Often triggered by frustration, disappointment, or\n",
      "\n",
      "Chunk 8:\n",
      " injustice.  \n",
      "4. **Fear** – A natural response to danger, uncertainty, or threat.  \n",
      "5. **Joy** – A positive emotion that comes from happiness, fulfillment, or a sense of purpose.  \n",
      "\n",
      "These emotions are deeply rooted in\n",
      "\n",
      "Chunk 9:\n",
      " human psychology and are frequently experienced in daily life. While emotions can vary across cultures and contexts, these are widely recognized as the most common emotional experiences.\n",
      "\n",
      "================================================================================\n",
      "COMPLETE RESPONSE:\n",
      "================================================================================\n",
      "<think>\n",
      "Okay, the user is asking about the most common emotions people feel. Let me start by recalling what I know. I remember that emotions can vary widely, but there are some that are more prevalent. For example, happiness and sadness are often mentioned. But wait, are there more?\n",
      "\n",
      "I think joy and anger are also common. But maybe the user wants a more comprehensive list. Let me think of other emotions too. Fear, anxiety, and fear are definitely common. But maybe the user is looking for the top 5 or so. \n",
      "\n",
      "Wait, the user might be interested in the most frequent emotions in everyday life. So, happiness is a big one. Then maybe sadness, anger, fear, joy, and anxiety. But I should check if there are any other emotions that are more common. \n",
      "\n",
      "I should also consider cultural factors. For example, in some cultures, anxiety might be less common, but in others, it's more. However, the question is about the most common emotions, so maybe it's better to focus on the most universally recognized ones. \n",
      "\n",
      "Hmm, maybe the answer should list happiness, sadness, joy, anger, fear, and anxiety. But I need to make sure these are the top ones. Let me confirm. Yes, those are the top emotions people experience. So the answer should include those, perhaps with a brief explanation of why they're common.\n",
      "</think>\n",
      "\n",
      "The most common emotions people experience are often categorized as follows:\n",
      "\n",
      "1. **Happiness** – Often described as feeling joy, satisfaction, or contentment.  \n",
      "2. **Sadness** – Commonly associated with loneliness, longing, or loss.  \n",
      "3. **Anger** – Often triggered by frustration, disappointment, or injustice.  \n",
      "4. **Fear** – A natural response to danger, uncertainty, or threat.  \n",
      "5. **Joy** – A positive emotion that comes from happiness, fulfillment, or a sense of purpose.  \n",
      "\n",
      "These emotions are deeply rooted in human psychology and are frequently experienced in daily life. While emotions can vary across cultures and contexts, these are widely recognized as the most common emotional experiences.\n",
      "\n",
      "================================================================================\n",
      "SCORE SUMMARY\n",
      "================================================================================\n",
      "Iteration 1: Chosen score = 0.0075\n",
      "Iteration 2: Chosen score = 0.0070\n",
      "Iteration 3: Chosen score = 0.0072\n",
      "Iteration 4: Chosen score = 0.0061\n",
      "Iteration 5: Chosen score = 0.0068\n",
      "Iteration 6: Chosen score = 0.0047\n",
      "Iteration 7: Chosen score = 0.0106\n",
      "Iteration 8: Chosen score = 0.0073\n",
      "Iteration 9: Chosen score = 0.0053\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"What are the most common emotions that people feel? Think for a while\"\n",
    "test_result, test_iterations = recursive_completion_with_rollouts(\n",
    "    test_prompt, \n",
    "    angry_embedding,\n",
    "    max_tokens=50, \n",
    "    temperature=0.3, \n",
    "    n_rollouts=30,\n",
    "    max_iterations=100\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "for i, chunk in enumerate(test_result):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(chunk)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE RESPONSE:\")\n",
    "print(\"=\"*80)\n",
    "print(''.join(test_result))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCORE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for iter_data in test_iterations:\n",
    "    print(f\"Iteration {iter_data['iteration']}: Chosen score = {iter_data['chosen_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b759669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading emotion examples from JSON...\n",
      "Loaded 101 angry examples and 100 neutral examples\n",
      "\n",
      "Computing scores for angry examples...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 17.38 MiB is free. Process 20811 has 72.54 GiB memory in use. Including non-PyTorch memory, this process has 6.69 GiB memory in use. Of the allocated memory 6.05 GiB is allocated by PyTorch, and 148.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Compute scores for all examples using batch processing for efficiency\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mComputing scores for angry examples...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m angry_scores = \u001b[43mcompute_emotion_scores_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mangry_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangry_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneutral_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mComputing scores for neutral examples...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m neutral_scores = compute_emotion_scores_batch(neutral_texts, angry_embedding, neutral_embedding)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 129\u001b[39m, in \u001b[36mcompute_emotion_scores_batch\u001b[39m\u001b[34m(texts, target_embedding, neutral_embedding, layer_idx)\u001b[39m\n\u001b[32m    125\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# Get all hidden states\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m     hidden_states = outputs.hidden_states\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# Extract from specified layer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:410\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    423\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    424\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    425\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:1023\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper.<locals>.make_capture_wrapper.<locals>.wrapped_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1021\u001b[39m         output = orig_forward(*args, **kwargs)\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     output = \u001b[43morig_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   1025\u001b[39m     collected_outputs[key] += (output,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:1023\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper.<locals>.make_capture_wrapper.<locals>.wrapped_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1021\u001b[39m         output = orig_forward(*args, **kwargs)\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     output = \u001b[43morig_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   1025\u001b[39m     collected_outputs[key] += (output,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:1023\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper.<locals>.make_capture_wrapper.<locals>.wrapped_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1021\u001b[39m         output = orig_forward(*args, **kwargs)\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     output = \u001b[43morig_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   1025\u001b[39m     collected_outputs[key] += (output,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:275\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    273\u001b[39m residual = hidden_states\n\u001b[32m    274\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:82\u001b[39m, in \u001b[36mQwen3MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/transformers/activations.py:99\u001b[39m, in \u001b[36mSiLUActivation.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/basic_science_resampling/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2375\u001b[39m, in \u001b[36msilu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   2373\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   2374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.silu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2375\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 17.38 MiB is free. Process 20811 has 72.54 GiB memory in use. Including non-PyTorch memory, this process has 6.69 GiB memory in use. Of the allocated memory 6.05 GiB is allocated by PyTorch, and 148.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Load the emotion examples from JSON\n",
    "print(\"Loading emotion examples from JSON...\")\n",
    "with open('emotion_examples.json', 'r') as f:\n",
    "    emotion_data = json.load(f)\n",
    "\n",
    "angry_texts = emotion_data['angry']\n",
    "neutral_texts = emotion_data['neutral']\n",
    "\n",
    "print(f\"Loaded {len(angry_texts)} angry examples and {len(neutral_texts)} neutral examples\")\n",
    "\n",
    "# Compute scores for all examples using batch processing for efficiency\n",
    "print(\"\\nComputing scores for angry examples...\")\n",
    "angry_scores = compute_emotion_scores_batch(angry_texts, angry_embedding, neutral_embedding)\n",
    "\n",
    "print(\"Computing scores for neutral examples...\")\n",
    "neutral_scores = compute_emotion_scores_batch(neutral_texts, angry_embedding, neutral_embedding)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate statistics\n",
    "angry_mean = np.mean(angry_scores)\n",
    "angry_std = np.std(angry_scores)\n",
    "angry_min = np.min(angry_scores)\n",
    "angry_max = np.max(angry_scores)\n",
    "\n",
    "neutral_mean = np.mean(neutral_scores)\n",
    "neutral_std = np.std(neutral_scores)\n",
    "neutral_min = np.min(neutral_scores)\n",
    "neutral_max = np.max(neutral_scores)\n",
    "\n",
    "print(f\"\\nAngry Examples (n={len(angry_scores)}):\")\n",
    "print(f\"  Mean:   {angry_mean:.6f}\")\n",
    "print(f\"  Std:    {angry_std:.6f}\")\n",
    "print(f\"  Min:    {angry_min:.6f}\")\n",
    "print(f\"  Max:    {angry_max:.6f}\")\n",
    "\n",
    "print(f\"\\nNeutral Examples (n={len(neutral_scores)}):\")\n",
    "print(f\"  Mean:   {neutral_mean:.6f}\")\n",
    "print(f\"  Std:    {neutral_std:.6f}\")\n",
    "print(f\"  Min:    {neutral_min:.6f}\")\n",
    "print(f\"  Max:    {neutral_max:.6f}\")\n",
    "\n",
    "print(f\"\\nDifference in Means: {angry_mean - neutral_mean:.6f}\")\n",
    "\n",
    "# Perform t-test to see if distributions are significantly different\n",
    "t_statistic, p_value = stats.ttest_ind(angry_scores, neutral_scores)\n",
    "print(f\"\\nt-test results:\")\n",
    "print(f\"  t-statistic: {t_statistic:.4f}\")\n",
    "print(f\"  p-value:     {p_value:.6e}\")\n",
    "print(f\"  Significant: {'YES' if p_value < 0.001 else 'NO'} (at α=0.001)\")\n",
    "\n",
    "# Calculate effect size (Cohen's d)\n",
    "pooled_std = np.sqrt(((len(angry_scores)-1)*angry_std**2 + (len(neutral_scores)-1)*neutral_std**2) / (len(angry_scores) + len(neutral_scores) - 2))\n",
    "cohens_d = (angry_mean - neutral_mean) / pooled_std\n",
    "print(f\"  Cohen's d:   {cohens_d:.4f}\")\n",
    "\n",
    "# Visualize the distributions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Histograms\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(angry_scores, bins=30, alpha=0.6, label='Angry', color='red', edgecolor='black')\n",
    "ax1.hist(neutral_scores, bins=30, alpha=0.6, label='Neutral', color='blue', edgecolor='black')\n",
    "ax1.axvline(angry_mean, color='red', linestyle='--', linewidth=2, label=f'Angry Mean: {angry_mean:.4f}')\n",
    "ax1.axvline(neutral_mean, color='blue', linestyle='--', linewidth=2, label=f'Neutral Mean: {neutral_mean:.4f}')\n",
    "ax1.set_xlabel('Angry Score (vs Neutral)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Angry Scores')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plots\n",
    "ax2 = axes[0, 1]\n",
    "box_data = [angry_scores, neutral_scores]\n",
    "bp = ax2.boxplot(box_data, labels=['Angry', 'Neutral'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('red')\n",
    "bp['boxes'][0].set_alpha(0.6)\n",
    "bp['boxes'][1].set_facecolor('blue')\n",
    "bp['boxes'][1].set_alpha(0.6)\n",
    "ax2.set_ylabel('Angry Score (vs Neutral)')\n",
    "ax2.set_title('Box Plot Comparison')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Violin plots\n",
    "ax3 = axes[1, 0]\n",
    "parts = ax3.violinplot([angry_scores, neutral_scores], positions=[1, 2], showmeans=True, showmedians=True)\n",
    "for pc, color in zip(parts['bodies'], ['red', 'blue']):\n",
    "    pc.set_facecolor(color)\n",
    "    pc.set_alpha(0.6)\n",
    "ax3.set_xticks([1, 2])\n",
    "ax3.set_xticklabels(['Angry', 'Neutral'])\n",
    "ax3.set_ylabel('Angry Score (vs Neutral)')\n",
    "ax3.set_title('Violin Plot Comparison')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Scatter plot with indices\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(range(len(angry_scores)), angry_scores, alpha=0.5, s=30, c='red', label='Angry')\n",
    "ax4.scatter(range(len(neutral_scores)), neutral_scores, alpha=0.5, s=30, c='blue', label='Neutral')\n",
    "ax4.axhline(angry_mean, color='red', linestyle='--', linewidth=1, alpha=0.7)\n",
    "ax4.axhline(neutral_mean, color='blue', linestyle='--', linewidth=1, alpha=0.7)\n",
    "ax4.axhline(0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "ax4.set_xlabel('Example Index')\n",
    "ax4.set_ylabel('Angry Score (vs Neutral)')\n",
    "ax4.set_title('Score Distribution by Example')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('emotion_score_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Saved visualization to: emotion_score_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "# Show some example texts with their scores\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE EXAMPLES WITH SCORES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- Top 10 Highest Scoring Angry Examples ---\")\n",
    "angry_with_scores = list(zip(angry_texts, angry_scores))\n",
    "angry_sorted = sorted(angry_with_scores, key=lambda x: x[1], reverse=True)\n",
    "for i, (text, score) in enumerate(angry_sorted[:10]):\n",
    "    print(f\"{i+1}. Score: {score:.6f} | Text: {text[:80]}...\")\n",
    "\n",
    "print(\"\\n--- Top 10 Lowest Scoring Angry Examples (should still be positive) ---\")\n",
    "for i, (text, score) in enumerate(angry_sorted[-10:]):\n",
    "    print(f\"{i+1}. Score: {score:.6f} | Text: {text[:80]}...\")\n",
    "\n",
    "print(\"\\n--- Top 10 Highest Scoring Neutral Examples (should be negative) ---\")\n",
    "neutral_with_scores = list(zip(neutral_texts, neutral_scores))\n",
    "neutral_sorted = sorted(neutral_with_scores, key=lambda x: x[1], reverse=True)\n",
    "for i, (text, score) in enumerate(neutral_sorted[:10]):\n",
    "    print(f\"{i+1}. Score: {score:.6f} | Text: {text[:80]}...\")\n",
    "\n",
    "print(\"\\n--- Top 10 Lowest Scoring Neutral Examples ---\")\n",
    "for i, (text, score) in enumerate(neutral_sorted[-10:]):\n",
    "    print(f\"{i+1}. Score: {score:.6f} | Text: {text[:80]}...\")\n",
    "\n",
    "# Classification accuracy (using 0 as threshold)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count correct classifications\n",
    "angry_correct = sum(1 for score in angry_scores if score > 0)\n",
    "neutral_correct = sum(1 for score in neutral_scores if score < 0)\n",
    "total_correct = angry_correct + neutral_correct\n",
    "total_samples = len(angry_scores) + len(neutral_scores)\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "\n",
    "print(f\"\\nUsing threshold = 0:\")\n",
    "print(f\"  Angry correctly classified:   {angry_correct}/{len(angry_scores)} ({angry_correct/len(angry_scores)*100:.1f}%)\")\n",
    "print(f\"  Neutral correctly classified: {neutral_correct}/{len(neutral_scores)} ({neutral_correct/len(neutral_scores)*100:.1f}%)\")\n",
    "print(f\"  Overall accuracy:             {total_correct}/{total_samples} ({accuracy*100:.1f}%)\")\n",
    "\n",
    "# Save results to JSON for future reference\n",
    "results = {\n",
    "    \"statistics\": {\n",
    "        \"angry\": {\n",
    "            \"mean\": float(angry_mean),\n",
    "            \"std\": float(angry_std),\n",
    "            \"min\": float(angry_min),\n",
    "            \"max\": float(angry_max)\n",
    "        },\n",
    "        \"neutral\": {\n",
    "            \"mean\": float(neutral_mean),\n",
    "            \"std\": float(neutral_std),\n",
    "            \"min\": float(neutral_min),\n",
    "            \"max\": float(neutral_max)\n",
    "        },\n",
    "        \"difference_in_means\": float(angry_mean - neutral_mean),\n",
    "        \"t_statistic\": float(t_statistic),\n",
    "        \"p_value\": float(p_value),\n",
    "        \"cohens_d\": float(cohens_d)\n",
    "    },\n",
    "    \"classification\": {\n",
    "        \"threshold\": 0.0,\n",
    "        \"angry_correct\": int(angry_correct),\n",
    "        \"angry_total\": len(angry_scores),\n",
    "        \"neutral_correct\": int(neutral_correct),\n",
    "        \"neutral_total\": len(neutral_scores),\n",
    "        \"accuracy\": float(accuracy)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('emotion_score_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"\\nSaved results to: emotion_score_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36ae15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
